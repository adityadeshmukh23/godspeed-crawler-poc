# ğŸ“„ PRD - Godspeed Crawler

## ğŸ¯ Objective
To build a file-based content ingestion system that pulls files from GitHub repositories and web URLs, storing them locally in structured folders. The system only syncs new or updated content to avoid redundancy and enable efficient downstream use (e.g., indexing, static generation, or AI pipelines).

---

## âœ… Features Implemented in POC (Proof of Concept)

- ğŸ” GitHub repository sync using GitHub API & commit hashes  
- ğŸ—‚ï¸ Folder structure mirrors the GitHub repoâ€™s layout  
- ğŸ“„ Files saved in `.md` format with **YAML frontmatter metadata**  
- ğŸ” Only new or modified files are written to disk  
- ğŸ” Secure access using token in `.env` (not committed)

---

## ğŸ§  Technical Approach

- ğŸ“¦ Used [`axios`](https://www.npmjs.com/package/axios) for HTTP requests  
- ğŸ“ [`fs-extra`](https://www.npmjs.com/package/fs-extra) for advanced file operations  
- ğŸ” [`dotenv`](https://www.npmjs.com/package/dotenv) to manage GitHub access token  
- ğŸ§® Used commit hash (SHA) to detect and sync only updated files  

---

â­ï¸ Future Scope

ğŸŒ Web content crawling (blogs, news) via RSS or sitemaps
ğŸ¦ Add Twitter/X feed support using Twitter API
â²ï¸ Add scheduled syncs using cron or godspeed-cron
ğŸ§ª Add duplicate detection via SHA-256 content hashing
âš™ï¸ Add CLI or REST API to trigger syncs manually

---

## ğŸ“ Output Folder Structure
```bash
data/
â””â”€â”€ github/
    â””â”€â”€ owner/
        â””â”€â”€ repo/
            â”œâ”€â”€ README.md
            â””â”€â”€ index.md



